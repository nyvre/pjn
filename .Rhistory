# change working directory
library("tm")
library("hunspell")
workDir <- "G:\\Informatyka_zaoczna\\PJN\\projekt"
setwd(workDir)
# functional catalogs localization definition
inputDir <- ".\\data"
outputDir <- ".\\results"
scriptsDir <- ".\\scripts"
#
dir.create(outputDir)
corpusDir <- paste(
inputDir,
"Artykuly_original",
sep = "\\"
)
corpus <- VCorpus(
DirSource(
corpusDir,
encoding = "UTF-8",
pattern = "*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# initial transformation
pasteParagraphs <- function (text) {
paste(text, collapse = " ")
}
corpus <- tm_map(corpus, content_transformer(pasteParagraphs))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
polish <- dictionary("G:\\Informatyka_zaoczna\\PJN\\pl_PL.dic")
text <-corpus[[1]]$content
tokenizedText <- unlist(hunspell_parse(text, dict = polish))
lematizedVec <- hunspell_stem(tokenizedText, dict = polish)
t <- 1
lematizedVec[[t]]
length(lematizedVec[[t]])
length(lematizedVec[[t]]) == 0
length(lematizedVec[[t]]) > 1
if (length(lematizedVec[[t]]) == 0) g <- 1
if (length(lematizedVec[[t]]) > 1) g <- 1
length(lematizedVec)
for (t in 1:length(lematizedVec)) {
print(t)
}
length(lematizedVec)
print(t)
for (t in 1:length(lematizedVec)) {
print(lematizedVec[t])
print(length(lematizedVec[t]))
if (length(lematizedVec[[t]]) == 0) lematizedVec[[t]] <- tokenizedText[[t]]
if (length(lematizedVec[[t]]) > 1) lematizedVec[[t]] <- lematizedVec[[t]][1]
lematizedText <- paste(lematizedVec, collapse = " ")
return(lematizedText)
}
# change working directory
library("tm")
library("hunspell")
workDir <- "G:\\Informatyka_zaoczna\\PJN\\projekt"
setwd(workDir)
# functional catalogs localization definition
inputDir <- ".\\data"
outputDir <- ".\\results"
scriptsDir <- ".\\scripts"
#
dir.create(outputDir)
corpusDir <- paste(
inputDir,
"Artykuly_original",
sep = "\\"
)
corpus <- VCorpus(
DirSource(
corpusDir,
encoding = "UTF-8",
pattern = "*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# initial transformation
pasteParagraphs <- function (text) {
paste(text, collapse = " ")
}
corpus <- tm_map(corpus, content_transformer(pasteParagraphs))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
polish <- dictionary("G:\\Informatyka_zaoczna\\PJN\\pl_PL.dic")
lematize <- function (text) {
text <-corpus[[1]]$content
tokenizedText <- unlist(hunspell_parse(text, dict = polish))
lematizedVec <- hunspell_stem(tokenizedText, dict = polish)
for (t in 1:length(lematizedVec)) {
print(lematizedVec[[t]])
print(length(lematizedVec[[t]]))
if (length(lematizedVec[[t]]) == 0) lematizedVec[[t]] <- tokenizedText[[t]]
if (length(lematizedVec[[t]]) > 1) lematizedVec[[t]] <- lematizedVec[[t]][1]
}
lematizedText <- paste(lematizedVec, collapse = " ")
return(lematizedText)
}
corpus <- tm_map(corpus, content_transformer(lematize))
